{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5721,
     "status": "ok",
     "timestamp": 1731658157078,
     "user": {
      "displayName": "KIdus mesele",
      "userId": "14631280397181120254"
     },
     "user_tz": 0
    },
    "id": "59o70n3Kza0I"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GT7DYDv50E9Q"
   },
   "outputs": [],
   "source": [
    "def load_data(train_path, store_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    store = pd.read_csv(store_path)\n",
    "    df = pd.merge(train, store, on='Store', how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pZYqIFH10J3z"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Convert date column to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Extract date-related features\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    # Feature: Beginning, Mid, End of Month\n",
    "    df['IsBeginningOfMonth'] = df['Day'].apply(lambda x: 1 if x <= 10 else 0)\n",
    "    df['IsMidMonth'] = df['Day'].apply(lambda x: 1 if 10 < x <= 20 else 0)\n",
    "    df['IsEndOfMonth'] = df['Day'].apply(lambda x: 1 if x > 20 else 0)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    df['StateHoliday'] = df['StateHoliday'].replace({'a': 1, 'b': 2, 'c': 3, '0': 0})\n",
    "    df['StoreType'] = df['StoreType'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n",
    "    df['Assortment'] = df['Assortment'].map({'a': 1, 'b': 2, 'c': 3})\n",
    "\n",
    "    # Handle 'PromoInterval' - Convert it to binary columns indicating the months\n",
    "    month_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "                 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "\n",
    "    for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']:\n",
    "        df[f'Promo_{month}'] = df['PromoInterval'].apply(lambda x: 1 if isinstance(x, str) and month in x else 0)\n",
    "\n",
    "    # Fill missing values\n",
    "    df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
    "    df['CompetitionOpenSinceYear'].fillna(df['CompetitionOpenSinceYear'].mode()[0], inplace=True)\n",
    "    df['Promo2SinceYear'].fillna(df['Promo2SinceYear'].mode()[0], inplace=True)\n",
    "    df.fillna(0, inplace=True)  # Fill remaining NaNs with 0\n",
    "\n",
    "    # Feature engineering\n",
    "    df['CompetitionOpenTime'] = 12 * (df['Year'] - df['CompetitionOpenSinceYear']) + (df['Month'] - df['CompetitionOpenSinceMonth'])\n",
    "    df['Promo2Time'] = 12 * (df['Year'] - df['Promo2SinceYear']) + (df['WeekOfYear'] - df['Promo2SinceWeek'])\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df.drop(columns=['Date', 'PromoInterval'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zdk0ZWhS0L25"
   },
   "outputs": [],
   "source": [
    "def scale_data(df, target_column='Sales'):\n",
    "    features = df.drop(columns=[target_column])\n",
    "    target = df[target_column]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    return scaled_features, target\n",
    "\n",
    "\n",
    "def build_pipeline():\n",
    "    # Create a pipeline with scaling and a random forest model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "24VdlXup0SfD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, target_column='Sales'):\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get the pipeline\n",
    "    pipeline = build_pipeline()\n",
    "\n",
    "    # Fit the pipeline to the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "o5NnFiVD0bA-"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(df_processed):\n",
    "    # Splitting data into train and test sets\n",
    "    X = df_processed.drop(columns=['Sales'])\n",
    "    y = df_processed['Sales']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building a pipeline with Random Forest Regressor\n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', RandomForestRegressor(criterion='absolute_error', random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0BSADaBi0gin"
   },
   "outputs": [],
   "source": [
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Get feature importance from the trained RandomForest model.\n",
    "    \"\"\"\n",
    "    importances = model.named_steps['regressor'].feature_importances_\n",
    "    feature_importance = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qkAQ4pwV0jVZ"
   },
   "outputs": [],
   "source": [
    "def estimate_confidence_interval(model, X_test, y_test, n_iterations=1000, alpha=0.95):\n",
    "    \"\"\"\n",
    "    Estimate confidence intervals using bootstrapping.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    n_size = X_test.shape[0]\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Resample the test data\n",
    "        X_resampled, y_resampled = resample(X_test, y_test, n_samples=n_size, random_state=42)\n",
    "\n",
    "        # Predict using the model\n",
    "        y_pred_resampled = model.predict(X_resampled)\n",
    "        predictions.append(y_pred_resampled)\n",
    "\n",
    "    # Convert predictions to numpy array for easy calculations\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    lower_bound = np.percentile(predictions, (1 - alpha) / 2 * 100, axis=0)\n",
    "    upper_bound = np.percentile(predictions, (1 + alpha) / 2 * 100, axis=0)\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YtgM4cPF0nNc"
   },
   "outputs": [],
   "source": [
    "def post_prediction_analysis(df_processed, model):\n",
    "    \"\"\"\n",
    "    Performs post-prediction analysis including feature importance and confidence interval estimation.\n",
    "    \"\"\"\n",
    "    # Splitting data into test set\n",
    "    X_test = df_processed.drop(columns=['Sales'])\n",
    "    y_test = df_processed['Sales']\n",
    "\n",
    "    # Feature importance\n",
    "    feature_names = X_test.columns\n",
    "    feature_importance = get_feature_importance(model, feature_names)\n",
    "\n",
    "    print(\"Feature Importance Ranking:\")\n",
    "    for feature, importance in feature_importance:\n",
    "        print(f\"{feature}: {importance}\")\n",
    "\n",
    "    # Confidence interval estimation\n",
    "    lower_bound, upper_bound = estimate_confidence_interval(model, X_test, y_test)\n",
    "\n",
    "    print(f\"Lower Bound of Predictions: {lower_bound}\")\n",
    "    print(f\"Upper Bound of Predictions: {upper_bound}\")\n",
    "\n",
    "    return feature_importance, lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c-KrFzwG0q1Q"
   },
   "outputs": [],
   "source": [
    "def serialize_model(model, folder_path='models/'):\n",
    "    \"\"\"\n",
    "    Serialize and save the model with a timestamped filename.\n",
    "    \"\"\"\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Generate the timestamped filename\n",
    "    timestamp = datetime.now().strftime('%d-%m-%Y-%H-%M-%S-%f')\n",
    "    filename = os.path.join(folder_path, f'{timestamp}.pkl')\n",
    "\n",
    "    # Save the model using joblib\n",
    "    joblib.dump(model, filename)\n",
    "\n",
    "    print(f\"Model serialized and saved as {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11679,
     "status": "ok",
     "timestamp": 1731607224986,
     "user": {
      "displayName": "KIdus mesele",
      "userId": "14631280397181120254"
     },
     "user_tz": 0
    },
    "id": "z36GILz90tNt",
    "outputId": "c86280a8-95fe-493e-ac27-5240fdcbe754"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0XJDT9IB1DRv"
   },
   "outputs": [],
   "source": [
    "# Define paths to the data files\n",
    "train_file = '../data/train.csv'\n",
    "store_file = '../data/store.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4195,
     "status": "ok",
     "timestamp": 1731607244712,
     "user": {
      "displayName": "KIdus mesele",
      "userId": "14631280397181120254"
     },
     "user_tz": 0
    },
    "id": "dSOVO4n12iHS",
    "outputId": "a813a253-b63a-4b32-e311-d1ae4265ec88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25201/2147692440.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(train_path)\n",
      "/tmp/ipykernel_25201/249695320.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['StateHoliday'] = df['StateHoliday'].replace({'a': 1, 'b': 2, 'c': 3, '0': 0})\n",
      "/tmp/ipykernel_25201/249695320.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
      "/tmp/ipykernel_25201/249695320.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['CompetitionOpenSinceYear'].fillna(df['CompetitionOpenSinceYear'].mode()[0], inplace=True)\n",
      "/tmp/ipykernel_25201/249695320.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Promo2SinceYear'].fillna(df['Promo2SinceYear'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = load_data(train_file, store_file)\n",
    "df_processed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fwd4UDYE2tah"
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaled_features, target = scale_data(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925580,
     "status": "ok",
     "timestamp": 1731608182307,
     "user": {
      "displayName": "KIdus mesele",
      "userId": "14631280397181120254"
     },
     "user_tz": 0
    },
    "id": "aTRp0HT22xMA",
    "outputId": "edc275df-0a01-4b4d-be6e-9dcfb7d2ba15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 439.28668619628246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "pipeline = train_and_evaluate(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0paq8ck86c_w"
   },
   "outputs": [],
   "source": [
    "mae = train_and_evaluate_model(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/kali/Desktop/Pharmaceutical_store_sales/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = df_processed.drop(columns=['Sales'])\n",
    "y = df_processed['Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Parameters to log\n",
    "    params = {\"n_estimators\": 100, \"max_depth\": 5}\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Log parameters, metrics, and model\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(\"Model logged in MLflow!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhmR0CAEBcYZXXuJMIcNPG",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
